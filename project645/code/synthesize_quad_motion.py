import os
import torch
import torch.nn as nn
import torch.nn.functional as F # For F.normalize in loss calculation
from torch.autograd import Variable
import numpy as np
import random
import read_bvh
import argparse
import json # For loading metadata
import read_bvh_hierarchy # For loading skeleton hierarchy
import rotation_conversions # For quaternion to euler conversions

# Default values, might be overridden by metadata or args
DEFAULT_HIDDEN_SIZE = 1024
DEFAULT_HIP_POS_SCALE_FACTOR = 0.01

# Constants for hip channel processing
HIP_X_CHANNEL, HIP_Y_CHANNEL, HIP_Z_CHANNEL = 0, 1, 2

# Global list for non_end_bones, loaded once
NON_END_BONES_LIST = []
# Global for original frame time from metadata
ORIGINAL_FRAME_TIME = 0.016667 # Default (60 FPS)

# Experiment Notes from pytorch_train_quad_aclstm.py regarding Condition_num and Groundtruth_num:
# The training script (pytorch_train_quad_aclstm.py) experimented with different
# Condition_num and Groundtruth_num values. These settings during training significantly
# impact the behavior of the synthesized motion generated by this script.
# 1. Training with Condition_num = 5, Groundtruth_num = 5: Resulted in synthesized motion of ~20 frames before stillness.
# 2. Training with Condition_num = 30, Groundtruth_num = 5: Resulted in synthesized motion of ~50 frames before stillness.
# 3. Training with Condition_num = 45, Groundtruth_num = 5: Resulted in synthesized motion for the full requested duration.
# This script uses the model trained with one of these configurations. The quality and length
# of continuous motion depend on these training-time parameters.

class acLSTM(nn.Module):
    def __init__(self, in_frame_size, hidden_size, out_frame_size):
        super(acLSTM, self).__init__()
        
        self.in_frame_size = in_frame_size
        self.hidden_size = hidden_size
        self.out_frame_size = out_frame_size
        
        self.lstm1 = nn.LSTMCell(self.in_frame_size, self.hidden_size)
        self.lstm2 = nn.LSTMCell(self.hidden_size, self.hidden_size)
        self.lstm3 = nn.LSTMCell(self.hidden_size, self.hidden_size)
        self.decoder = nn.Linear(self.hidden_size, self.out_frame_size)
    
    def init_hidden(self, batch_size):
        device = next(self.parameters()).device
        c0 = Variable(torch.FloatTensor(np.zeros((batch_size, self.hidden_size))).to(device))
        c1 = Variable(torch.FloatTensor(np.zeros((batch_size, self.hidden_size))).to(device))
        c2 = Variable(torch.FloatTensor(np.zeros((batch_size, self.hidden_size))).to(device))
        h0 = Variable(torch.FloatTensor(np.zeros((batch_size, self.hidden_size))).to(device))
        h1 = Variable(torch.FloatTensor(np.zeros((batch_size, self.hidden_size))).to(device))
        h2 = Variable(torch.FloatTensor(np.zeros((batch_size, self.hidden_size))).to(device))
        return ([h0,h1,h2], [c0,c1,c2])
    
    def forward_lstm(self, in_frame, vec_h, vec_c):
        vec_h0, vec_c0 = self.lstm1(in_frame, (vec_h[0], vec_c[0]))
        vec_h1, vec_c1 = self.lstm2(vec_h0, (vec_h[1], vec_c[1]))
        vec_h2, vec_c2 = self.lstm3(vec_h1, (vec_h[2], vec_c[2]))
        out_frame = self.decoder(vec_h2)
        return (out_frame, [vec_h0, vec_h1, vec_h2], [vec_c0, vec_c1, vec_c2])
        
    def forward(self, initial_seq_processed_lstm, generate_frames_number):
        batch_size = initial_seq_processed_lstm.size(0)
        initial_processed_len = initial_seq_processed_lstm.size(1)

        (vec_h, vec_c) = self.init_hidden(batch_size)
        
        out_seq_list = []
        current_frame_for_generation = None
        device = next(self.parameters()).device

        if initial_processed_len > 0:
            for i in range(initial_processed_len):
                in_frame = initial_seq_processed_lstm[:, i, :]
                (out_frame, vec_h, vec_c) = self.forward_lstm(in_frame, vec_h, vec_c)
                if i == initial_processed_len - 1:
                    current_frame_for_generation = out_frame
        else: 
            current_frame_for_generation = torch.zeros(batch_size, self.out_frame_size, device=device)

        for _ in range(generate_frames_number):
            in_frame = current_frame_for_generation 
            (out_frame, vec_h, vec_c) = self.forward_lstm(in_frame, vec_h, vec_c)
            out_seq_list.append(out_frame.unsqueeze(1))
            current_frame_for_generation = out_frame
    
        if not out_seq_list:
            return torch.zeros(batch_size, 0, self.out_frame_size, device=device)

        return torch.cat(out_seq_list, 1)

    def calculate_loss(self, pred_seq_raw, groundtruth_seq_processed_for_loss):
        hip_pos_pred = pred_seq_raw[..., :3]
        hip_pos_gt = groundtruth_seq_processed_for_loss[..., :3]
        
        quat_pred = pred_seq_raw[..., 3:]
        quat_gt = groundtruth_seq_processed_for_loss[..., 3:]

        loss_fn_mse = nn.MSELoss()
        loss_hip = loss_fn_mse(hip_pos_pred, hip_pos_gt)

        quat_pred_reshaped = quat_pred.reshape(-1, 4)
        quat_gt_reshaped = quat_gt.reshape(-1, 4)

        quat_pred_norm = F.normalize(quat_pred_reshaped, p=2, dim=-1)
        quat_gt_norm = F.normalize(quat_gt_reshaped, p=2, dim=-1)

        dot_product = torch.sum(quat_pred_norm * quat_gt_norm, dim=-1)
        # Clamp dot_product to avoid NaNs from acos due to floating point inaccuracies
        abs_dot_product_clamped = torch.clamp(torch.abs(dot_product), min=0.0, max=1.0 - 1e-7) 
        
        loss_quat_values = 2.0 * torch.acos(abs_dot_product_clamped)
        loss_rot_ql = torch.mean(loss_quat_values)
        
        total_loss = loss_hip + loss_rot_ql 
        return total_loss, loss_hip, loss_rot_ql


def convert_lstm_output_to_bvh_frames(scaled_lstm_output_np, hip_pos_scale_factor, num_data_channels):
    """
    Converts LSTM output (scaled hip diffs, scaled Y abs, absolute quaternions)
    to BVH-ready frames (unscaled absolute hip, Euler angles in degrees).
    The hip X,Z path is reconstructed relative to the start of this segment.
    """
    global NON_END_BONES_LIST
    
    bvh_frames_list = []
    num_frames = scaled_lstm_output_np.shape[0]

    current_abs_scaled_x_relative_to_segment_start = 0.0
    current_abs_scaled_z_relative_to_segment_start = 0.0
    
    num_rotating_joints_inc_hip = (num_data_channels - 3) // 4

    for frame_idx in range(num_frames):
        lstm_frame = scaled_lstm_output_np[frame_idx]
        bvh_frame_channels = []
        
        current_abs_scaled_x_relative_to_segment_start += lstm_frame[HIP_X_CHANNEL]
        current_abs_scaled_z_relative_to_segment_start += lstm_frame[HIP_Z_CHANNEL]
        
        unscaled_hip_x = current_abs_scaled_x_relative_to_segment_start / hip_pos_scale_factor
        unscaled_hip_y = lstm_frame[HIP_Y_CHANNEL] / hip_pos_scale_factor
        unscaled_hip_z = current_abs_scaled_z_relative_to_segment_start / hip_pos_scale_factor
        bvh_frame_channels.extend([unscaled_hip_x, unscaled_hip_y, unscaled_hip_z])
        
        quat_feature_cursor = 3
        
        hip_quat_wxyz = torch.tensor(lstm_frame[quat_feature_cursor : quat_feature_cursor+4], dtype=torch.float32)
        hip_rot_matrix = rotation_conversions.quaternion_to_matrix(hip_quat_wxyz.unsqueeze(0))
        hip_euler_rad_zyx = rotation_conversions.matrix_to_euler_angles(hip_rot_matrix, "ZYX").squeeze(0)
        hip_euler_deg_zyx = np.rad2deg(hip_euler_rad_zyx.numpy())
        bvh_frame_channels.extend(hip_euler_deg_zyx)
        quat_feature_cursor += 4
        
        for _ in range(num_rotating_joints_inc_hip - 1): # Matches non_end_bones expectation if data is consistent
            if quat_feature_cursor + 4 > len(lstm_frame):
                print(f"Error: Out of bounds quaternion access. Cursor: {quat_feature_cursor}, Frame len: {len(lstm_frame)}")
                bvh_frame_channels.extend([0.0, 0.0, 0.0]) 
                break
            joint_quat_wxyz = torch.tensor(lstm_frame[quat_feature_cursor : quat_feature_cursor+4], dtype=torch.float32)
            joint_rot_matrix = rotation_conversions.quaternion_to_matrix(joint_quat_wxyz.unsqueeze(0))
            joint_euler_rad_zxy = rotation_conversions.matrix_to_euler_angles(joint_rot_matrix, "ZXY").squeeze(0)
            joint_euler_deg_zxy = np.rad2deg(joint_euler_rad_zxy.numpy())
            bvh_frame_channels.extend(joint_euler_deg_zxy)
            quat_feature_cursor += 4
            
        bvh_frames_list.append(bvh_frame_channels)
        
    return np.array(bvh_frames_list, dtype=np.float32)

def generate_seq(initial_seq_np, generate_frames_number, model, args, metadata,
                 ground_truth_continuation_np=None, quantitative_comparison_len=0,
                 full_ground_truth_for_bvh_np=None):

    hip_pos_scale_factor = metadata.get("hip_pos_scale_factor", DEFAULT_HIP_POS_SCALE_FACTOR)
    num_data_channels = metadata.get("num_channels", model.out_frame_size)
    global ORIGINAL_FRAME_TIME

    if initial_seq_np.shape[1] < 2:
        print("Error: initial_seq_np must have at least 2 frames for diff calculation.")
        return np.array([]) 

    dif_hip_x_scaled = initial_seq_np[:, 1:, HIP_X_CHANNEL] - initial_seq_np[:, :-1, HIP_X_CHANNEL]
    dif_hip_z_scaled = initial_seq_np[:, 1:, HIP_Z_CHANNEL] - initial_seq_np[:, :-1, HIP_Z_CHANNEL]
    
    initial_seq_processed_for_lstm_np = initial_seq_np[:, :-1].copy()
    initial_seq_processed_for_lstm_np[:, :, HIP_X_CHANNEL] = dif_hip_x_scaled
    initial_seq_processed_for_lstm_np[:, :, HIP_Z_CHANNEL] = dif_hip_z_scaled

    device = next(model.parameters()).device
    initial_seq_processed_for_lstm_torch = torch.autograd.Variable(torch.FloatTensor(initial_seq_processed_for_lstm_np)).to(device)
 
    predict_seq_raw_torch = model.forward(initial_seq_processed_for_lstm_torch, generate_frames_number)
    predict_seq_raw_np = predict_seq_raw_torch.data.cpu().numpy() 
    
    generated_motion_all_batches_raw_output = []
   
    for b in range(initial_seq_np.shape[0]):
        current_batch_model_output_np = predict_seq_raw_np[b]
        generated_motion_all_batches_raw_output.append(current_batch_model_output_np.copy())

        last_seed_frame_scaled_abs_hip_x = initial_seq_np[b, -1, HIP_X_CHANNEL]
        last_seed_frame_scaled_abs_hip_z = initial_seq_np[b, -1, HIP_Z_CHANNEL]

        bvh_segment_generated_relative = convert_lstm_output_to_bvh_frames(current_batch_model_output_np, hip_pos_scale_factor, num_data_channels)
        
        bvh_ready_generated_motion = bvh_segment_generated_relative.copy()
        # Add the last seed frame's unscaled absolute position to the generated segment's unscaled relative positions
        bvh_ready_generated_motion[:, HIP_X_CHANNEL] += (last_seed_frame_scaled_abs_hip_x / hip_pos_scale_factor)
        bvh_ready_generated_motion[:, HIP_Z_CHANNEL] += (last_seed_frame_scaled_abs_hip_z / hip_pos_scale_factor)
        # Y position is already absolute (unscaled) from convert_lstm_output_to_bvh_frames
        
        try:
            read_bvh.write_frames(args.standard_bvh_file, 
                                  os.path.join(args.write_bvh_motion_folder, f"out{b:02d}.bvh"), 
                                  bvh_ready_generated_motion,
                                  frame_time_override=ORIGINAL_FRAME_TIME)
        except Exception as e:
            print(f"ERROR during generated BVH writing for batch {b}: {e}")

        if full_ground_truth_for_bvh_np is not None and b < full_ground_truth_for_bvh_np.shape[0]:
            gt_frames_for_bvh_raw = full_ground_truth_for_bvh_np[b]
            
            bvh_ready_gt_motion = gt_frames_for_bvh_raw.copy() # Hip X,Z relative to start, Y abs; all scaled. Quats abs.
            
            bvh_ready_gt_motion[:, HIP_X_CHANNEL] /= hip_pos_scale_factor
            bvh_ready_gt_motion[:, HIP_Y_CHANNEL] /= hip_pos_scale_factor
            bvh_ready_gt_motion[:, HIP_Z_CHANNEL] /= hip_pos_scale_factor
            
            quat_part_gt = bvh_ready_gt_motion[:, 3:]
            euler_part_gt_list = []
            
            for frame_idx_gt in range(quat_part_gt.shape[0]):
                gt_frame_quats = quat_part_gt[frame_idx_gt]
                gt_frame_eulers = []
                q_cursor = 0
                hip_q_wxyz = torch.tensor(gt_frame_quats[q_cursor : q_cursor+4], dtype=torch.float32)
                hip_r_mat = rotation_conversions.quaternion_to_matrix(hip_q_wxyz.unsqueeze(0))
                hip_e_rad = rotation_conversions.matrix_to_euler_angles(hip_r_mat, "ZYX").squeeze(0)
                gt_frame_eulers.extend(np.rad2deg(hip_e_rad.numpy()))
                q_cursor += 4
                for _ in range(len(NON_END_BONES_LIST)):
                    if q_cursor + 4 > len(gt_frame_quats): break # Safety
                    joint_q_wxyz = torch.tensor(gt_frame_quats[q_cursor: q_cursor+4], dtype=torch.float32)
                    joint_r_mat = rotation_conversions.quaternion_to_matrix(joint_q_wxyz.unsqueeze(0))
                    joint_e_rad = rotation_conversions.matrix_to_euler_angles(joint_r_mat, "ZXY").squeeze(0)
                    gt_frame_eulers.extend(np.rad2deg(joint_e_rad.numpy()))
                    q_cursor += 4
                euler_part_gt_list.append(gt_frame_eulers)
            
            if euler_part_gt_list: # Ensure list is not empty
                 hip_positions_gt = bvh_ready_gt_motion[:, :3]
                 euler_rotations_gt = np.array(euler_part_gt_list)
                 bvh_ready_gt_motion = np.concatenate((hip_positions_gt, euler_rotations_gt), axis=1)

            try:
                read_bvh.write_frames(args.standard_bvh_file,
                                      os.path.join(args.write_bvh_motion_folder, f"gt{b:02d}.bvh"),
                                      bvh_ready_gt_motion,
                                      frame_time_override=ORIGINAL_FRAME_TIME)
            except Exception as e:
                print(f"ERROR during GT BVH writing for batch {b}: {e}")
        
        if ground_truth_continuation_np is not None and quantitative_comparison_len > 0 and \
           b < ground_truth_continuation_np.shape[0]:
            
            if current_batch_model_output_np.shape[0] >= quantitative_comparison_len:
                generated_to_compare_raw = current_batch_model_output_np[0:quantitative_comparison_len, :]
                gt_raw_segment_for_loss = ground_truth_continuation_np[b, 0:quantitative_comparison_len, :]

                last_seed_frame_np = initial_seq_np[b, -1, :].reshape(1, -1)
                gt_extended_for_diff_calc = np.vstack((last_seed_frame_np, gt_raw_segment_for_loss))
                
                gt_diff_hip_x = gt_extended_for_diff_calc[1:, HIP_X_CHANNEL] - gt_extended_for_diff_calc[:-1, HIP_X_CHANNEL]
                gt_diff_hip_z = gt_extended_for_diff_calc[1:, HIP_Z_CHANNEL] - gt_extended_for_diff_calc[:-1, HIP_Z_CHANNEL]
                
                gt_processed_for_loss = gt_raw_segment_for_loss.copy()
                gt_processed_for_loss[:, HIP_X_CHANNEL] = gt_diff_hip_x
                gt_processed_for_loss[:, HIP_Z_CHANNEL] = gt_diff_hip_z
                
                pred_torch = torch.tensor(generated_to_compare_raw, dtype=torch.float32).unsqueeze(0).to(device)
                gt_torch = torch.tensor(gt_processed_for_loss, dtype=torch.float32).unsqueeze(0).to(device)
                
                eval_total_loss, eval_hip_loss, eval_rot_loss = model.calculate_loss(pred_torch, gt_torch)
                print(f"Batch {b}: Quant Eval (first {quantitative_comparison_len} frames) - "\
                      f"Total Loss: {eval_total_loss.item():.4f}, "\
                      f"Hip MSE: {eval_hip_loss.item():.4f}, "\
                      f"Rot Quat Loss: {eval_rot_loss.item():.4f}")
            else:
                print(f"Batch {b}: Not enough generated frames for quant comparison of length {quantitative_comparison_len}.")

    return np.array(generated_motion_all_batches_raw_output)

def get_dance_selection_indices(dances):
    index_lst = []
    for i, dance_frames in enumerate(dances):
        occurrences = max(1, dance_frames.shape[0] // 100) 
        index_lst.extend([i] * occurrences)
    return index_lst

def load_dances_and_metadata(dance_folder, metadata_path, model_expected_channels):
    dances = []
    metadata = {}
    
    try:
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        print(f"Loaded metadata from {metadata_path}")
    except Exception as e:
        print(f"Error loading metadata from {metadata_path}: {e}. Cannot proceed.")
        return None, None 
        
    if "num_channels" not in metadata:
        print("Error: 'num_channels' not found in metadata. Cannot validate data.")
        return None, metadata 

    if metadata["num_channels"] != model_expected_channels:
        print(f"CRITICAL Error: Metadata num_channels ({metadata['num_channels']}) "\
              f"does not match model's expected channels ({model_expected_channels}).")
        return None, metadata

    dance_files = sorted([f for f in os.listdir(dance_folder) if f.endswith(".npy")])
    print(f'Loading motion files from {dance_folder}...')
    for dance_file in dance_files:
        try:
            dance = np.load(os.path.join(dance_folder, dance_file))
            if dance.ndim == 2 and dance.shape[0] > 0 and dance.shape[1] == metadata["num_channels"]:
                dances.append(dance)
                print(f"Loaded {dance_file}, frames: {dance.shape[0]}")
            else:
                print(f"Warning: Skipping invalid dance file: {dance_file}. Shape: {dance.shape}, Expected channels: {metadata['num_channels']}")
        except Exception as e:
            print(f"Warning: Could not load dance file {dance_file}: {e}")
    print(f'{len(dances)} motion files loaded.')
    return dances, metadata

def main():
    parser = argparse.ArgumentParser(description="Synthesize Quaternion motion using a trained acLSTM model.")
    parser.add_argument('--read_weight_path', type=str, required=True, help='Path to the trained model weights (.weight file)')
    parser.add_argument('--dances_folder', type=str, required=True, help='Path to folder with seed .npy files (quaternion data, e.g., .../train_data_quad/salsa/)')
    parser.add_argument('--metadata_path', type=str, required=True, help='Path to metadata_quad.json for the dances_folder')
    parser.add_argument('--write_bvh_motion_folder', type=str, required=True, help='Path to folder to save generated BVH files')
    parser.add_argument('--standard_bvh_file', type=str, required=True, help='Path to standard.bvh for hierarchy')
    
    parser.add_argument('--batch_size', type=int, default=1, help='Number of sequences to generate')
    parser.add_argument('--initial_seq_len', type=int, default=20, help='Length of initial seed motion sequence (frames from .npy). Min 2.')
    parser.add_argument('--generate_frames_number', type=int, default=400, help='Number of frames to synthesize')
    
    parser.add_argument('--hidden_size', type=int, default=DEFAULT_HIDDEN_SIZE, help='LSTM hidden size')
    parser.add_argument('--quantitative_comparison_len', type=int, default=20, help='Num frames for quantitative loss against GT')

    args = parser.parse_args()

    if args.initial_seq_len < 2:
        print("Error: --initial_seq_len must be at least 2.")
        exit()

    os.makedirs(args.write_bvh_motion_folder, exist_ok=True)
    for path_arg, desc in [(args.standard_bvh_file, "Standard BVH file"), 
                           (args.read_weight_path, "Trained weight file"), 
                           (args.metadata_path, "Metadata file")]:
        if not os.path.exists(path_arg):
            print(f"Error: {desc} not found: {path_arg}")
            exit()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    global NON_END_BONES_LIST
    try:
        _, NON_END_BONES_LIST = read_bvh_hierarchy.read_bvh_hierarchy(args.standard_bvh_file)
    except Exception as e:
        print(f"Error loading BVH hierarchy from {args.standard_bvh_file}: {e}")
        return

    temp_metadata = {}
    try:
        with open(args.metadata_path, 'r') as f: temp_metadata = json.load(f)
    except Exception as e:
        print(f"Failed to load metadata from {args.metadata_path}: {e}"); exit()

    if "num_channels" not in temp_metadata:
        print(f"Critical: 'num_channels' not in metadata {args.metadata_path}."); exit()
    
    model_frame_channels = temp_metadata["num_channels"]
    global ORIGINAL_FRAME_TIME
    ORIGINAL_FRAME_TIME = temp_metadata.get("frame_time_original", 0.016667)


    dances_list, metadata = load_dances_and_metadata(args.dances_folder, args.metadata_path, model_frame_channels)
    if not dances_list or not metadata: print(f"Error loading dances/metadata. Exiting."); exit()

    dance_selection_indices = get_dance_selection_indices(dances_list)
    if not dance_selection_indices: print("Error: No dances for selection. Exiting."); exit()

    source_fps = 1.0 / ORIGINAL_FRAME_TIME if ORIGINAL_FRAME_TIME > 0 else 60.0
    MODEL_TARGET_FPS = 30.0 
    speed_factor = source_fps / MODEL_TARGET_FPS

    seed_batch_list, gt_continuation_batch_list, gt_full_bvh_batch_list = [], [], []
    max_gt_needed_model_fps = max(args.generate_frames_number, args.quantitative_comparison_len)
    min_req_source_frames = int((args.initial_seq_len + max_gt_needed_model_fps) * speed_factor) + 20

    for b_idx in range(args.batch_size):
        selected_dance_full, dance_log_id = None, -1
        # Simplified selection logic: try random, then longest if fails
        dance_indices_shuffled = random.sample(dance_selection_indices, len(dance_selection_indices))
        for idx_in_dances_list in dance_indices_shuffled:
             if dances_list[idx_in_dances_list].shape[0] >= min_req_source_frames:
                selected_dance_full = dances_list[idx_in_dances_list].copy(); dance_log_id = idx_in_dances_list; break
        if selected_dance_full is None:
            dances_list.sort(key=len, reverse=True) # Sort by length descending
            if dances_list and dances_list[0].shape[0] >= int(args.initial_seq_len * speed_factor) +2 : # Min for seed part
                selected_dance_full = dances_list[0].copy(); dance_log_id = "longest_fallback"
                print(f"Warning Batch {b_idx}: No dance long enough. Using longest ({selected_dance_full.shape[0]} frames).")
            else: print(f"Error Batch {b_idx}: No suitable dance. Skipping."); continue
        
        source_len = selected_dance_full.shape[0]
        total_segment_needed_source = int((args.initial_seq_len + max_gt_needed_model_fps) * speed_factor)
        max_start_src = max(0, source_len - total_segment_needed_source)
        start_src = random.randint(0, max_start_src)
        
        print(f"Batch {b_idx}: Dance {dance_log_id} (len {source_len}), start source frame {start_src}")

        seed_frames = [selected_dance_full[min(int(i * speed_factor + start_src), source_len-1)] for i in range(args.initial_seq_len)]
        seed_batch_list.append(np.array(seed_frames))

        gt_bvh_start_src = int(args.initial_seq_len * speed_factor + start_src)
        gt_bvh_frames = [selected_dance_full[min(int(i*speed_factor + gt_bvh_start_src), source_len-1)] for i in range(args.generate_frames_number)]
        gt_full_bvh_batch_list.append(np.array(gt_bvh_frames))

        if args.quantitative_comparison_len > 0:
            gt_quant_start_src = int(args.initial_seq_len * speed_factor + start_src)
            gt_quant_frames = [selected_dance_full[min(int(i*speed_factor+gt_quant_start_src), source_len-1)] for i in range(args.quantitative_comparison_len)]
            gt_continuation_batch_list.append(np.array(gt_quant_frames))

    if not seed_batch_list: print("Error: No seed sequences. Exiting."); exit()
    
    seed_batch_np = np.array(seed_batch_list)
    gt_continuation_np = np.array(gt_continuation_batch_list) if gt_continuation_batch_list else None
    # Ensure correct shape for gt_continuation_np if it's not None and batch_size=1 led to 2D array
    if gt_continuation_np is not None and gt_continuation_np.ndim == 2 and args.batch_size == 1 and args.quantitative_comparison_len > 0:
        gt_continuation_np = gt_continuation_np.reshape(1, args.quantitative_comparison_len, -1)
    elif gt_continuation_np is not None and gt_continuation_np.shape[0] != args.batch_size and gt_continuation_np.size > 0 : # If list was not fully populated
        print(f"Warning: GT continuation data has {gt_continuation_np.shape[0]} items, expected {args.batch_size}. Padding with zeros for loss calculation if needed.")
        # This case needs careful handling if some batch items failed seed selection
        # For now, assume if gt_continuation_np exists, it matches batch size or is None

    gt_full_bvh_np = np.array(gt_full_bvh_batch_list) if gt_full_bvh_batch_list else None
    if gt_full_bvh_np is not None and gt_full_bvh_np.ndim == 2 and args.batch_size == 1: # Reshape if batch_size 1
         gt_full_bvh_np = gt_full_bvh_np.reshape(1, args.generate_frames_number, -1)


    model = acLSTM(in_frame_size=model_frame_channels, hidden_size=args.hidden_size, out_frame_size=model_frame_channels)
    model.load_state_dict(torch.load(args.read_weight_path, map_location=device))
    model.to(device); model.eval()

    _ = generate_seq(seed_batch_np, args.generate_frames_number, model, args, metadata,
                     ground_truth_continuation_np=gt_continuation_np, 
                     quantitative_comparison_len=args.quantitative_comparison_len,
                     full_ground_truth_for_bvh_np=gt_full_bvh_np)
    
    print(f"Quaternion motion synthesis complete. BVH files saved to: {args.write_bvh_motion_folder}")

if __name__ == '__main__':
    main()