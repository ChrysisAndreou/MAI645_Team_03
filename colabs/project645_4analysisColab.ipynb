{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setting up data and env"],"metadata":{"id":"nBuehlq35L4x"}},{"cell_type":"markdown","source":["## Upload Your Project"],"metadata":{"id":"zK_iE6Rb219f"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"GWfv5Zzc2eZv","outputId":"6630027d-45ac-4d8d-c415-19c8b577bc4c","executionInfo":{"status":"ok","timestamp":1747848770606,"user_tz":-180,"elapsed":192323,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Upload your project645.zip file...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-815e8ade-a4b9-480f-b53f-4aa7dd1be51d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-815e8ade-a4b9-480f-b53f-4aa7dd1be51d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving project645.zip to project645.zip\n","User uploaded file \"project645.zip\" with length 24124184 bytes\n"]}],"source":["from google.colab import files\n","print(\"Upload your project645.zip file...\")\n","uploaded = files.upload()\n","\n","# Verify upload\n","for fn in uploaded.keys():\n","  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"]},{"cell_type":"markdown","source":["## Unzip the project"],"metadata":{"id":"wKZyBp5y3dhD"}},{"cell_type":"code","source":["!unzip -q project645.zip -d /content/\n","# Verify unzip by listing contents\n","!ls /content/project645/\n","!ls /content/project645/code/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CaZYeN_63YNo","executionInfo":{"status":"ok","timestamp":1747848792228,"user_tz":-180,"elapsed":969,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"ea9783aa-c6ac-49e8-9902-df90a4a43457"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["code  LICENSE  mai645.yml  README.md  train_data_bvh\n","analysis.py\t\t\t read_bvh.py\n","fix_feet.py\t\t\t read_bvh.pyc\n","generate_training_euler_data.py  rotation2xyz.py\n","generate_training_pos_data.py\t rotation2xyz.pyc\n","generate_training_quad_data.py\t rotation_conversions.py\n","__pycache__\t\t\t synthesize_euler_motion.py\n","pytorch_train_euler_aclstm.py\t synthesize_pos_motion_original_colab.py\n","pytorch_train_pos_aclstm.py\t synthesize_pos_motion.py\n","pytorch_train_quad_aclstm.py\t synthesize_quad_motion_debug.py\n","read_bvh_hierarchy.py\t\t synthesize_quad_motion.py\n","read_bvh_hierarchy.pyc\t\t test_encodings.py\n"]}]},{"cell_type":"markdown","source":["## Mount Google Drive"],"metadata":{"id":"F7XrtRYE3lSl"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSBSPgkp3nqG","executionInfo":{"status":"ok","timestamp":1747848818667,"user_tz":-180,"elapsed":24190,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"fd65de33-16ab-4f18-a152-0bb19e825b12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Create Output Folder"],"metadata":{"id":"HGWxCQaAfKCC"}},{"cell_type":"markdown","source":["*   **Create an Output Folder in Google Drive:** It's highly recommended to create a dedicated folder in your Google Drive to store the outputs (trained models, generated data/BVH files). For example, you could create a folder named `mai645_project_outputs` directly in your \"My Drive\".\n","*   Define a variable in Colab pointing to this Drive folder. **Make sure the path matches the folder you created.**"],"metadata":{"id":"bcZi7xIM33-g"}},{"cell_type":"markdown","source":["## **** WARNING YOU NEED TO MANUALLY ADD THE PREPROCCESSED DATA FOR QUADS AND POS IN THE FOLDER mai645_project_outputs_analysis AND RUN ONLY THE LOADING DATA STEPS BELOW FOR THE PATHS TO WORK AS EXPECTED****"],"metadata":{"id":"wHeXAIzVCp5I"}},{"cell_type":"code","source":["# IMPORTANT: Adjust this path if you named your Drive folder differently!\n","GDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/mai645_project_outputs_analysis\"\n","# Create the directory in Drive if it doesn't exist (optional, Colab can create it)\n","import os\n","os.makedirs(GDRIVE_OUTPUT_DIR, exist_ok=True)\n","print(f\"Outputs will be saved to: {GDRIVE_OUTPUT_DIR}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbv-wNTk34sD","executionInfo":{"status":"ok","timestamp":1747848831814,"user_tz":-180,"elapsed":4,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"9b8b9ff5-5830-49d1-a33d-7e91c43644c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Outputs will be saved to: /content/drive/MyDrive/mai645_project_outputs_analysis\n"]}]},{"cell_type":"markdown","source":["## Install Dependencies"],"metadata":{"id":"K6fxAysh4Aza"}},{"cell_type":"code","source":["# Install the latest compatible PyTorch, torchvision, torchaudio for this Colab environment\n","!pip install -q torch torchvision torchaudio\n","\n","# Install other libraries, specifically targeting numpy in the 2.0.x range.\n","# This aims to satisfy thinc (>=2.0.0) AND tensorflow/numba (<2.1.0).\n","!pip install -q contourpy==1.3.1 cycler==0.12.1 fonttools==4.56.0 kiwisolver==1.4.8 matplotlib==3.10.1 \"numpy>=2.0.0,<2.1.0\" opencv-python==4.11.0.86 packaging==24.2 pyparsing==3.2.3 python-dateutil==2.9.0.post0 six==1.17.0 transforms3d==0.4.2\n","\n","# Verify installation\n","!pip show torch torchvision torchaudio transforms3d numpy opencv-python tensorflow numba"],"metadata":{"id":"iiDk6zZ34Ch8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Change the current working directory to where your Python scripts are located"],"metadata":{"id":"lUWhlJaS5DJy"}},{"cell_type":"code","source":["cd /content/project645/code/"],"metadata":{"id":"-PoyGBYI5C88","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747848933796,"user_tz":-180,"elapsed":5,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"410207f9-bc7a-4243-838e-9632fdd8c14c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/project645/code\n"]}]},{"cell_type":"markdown","source":["# Positional Preprocessing"],"metadata":{"id":"x1OQNv2P5UGG"}},{"cell_type":"markdown","source":["## Upload preprocessed positional data"],"metadata":{"id":"uD26uguFZYAU"}},{"cell_type":"code","source":["import os\n","# Ensure GDRIVE_OUTPUT_DIR is defined (should match your Drive output folder)\n","# GDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/mai645_project_outputs\" # Uncomment and adjust if not run in a previous cell\n","\n","# Define source directories in Google Drive for positional data\n","GDRIVE_POS_TRAIN_DATA_SOURCE_DIR = os.path.join(GDRIVE_OUTPUT_DIR, \"train_data_pos\", \"salsa\")\n","GDRIVE_POS_RECON_BVH_SOURCE_DIR = os.path.join(GDRIVE_OUTPUT_DIR, \"reconstructed_bvh_data_pos\", \"salsa\")\n","\n","# Define target Colab directories\n","COLAB_POS_TRAIN_DATA_DIR = \"/content/project645/train_data_pos/salsa\"\n","COLAB_POS_RECON_BVH_DIR = \"/content/project645/reconstructed_bvh_data_pos/salsa\"\n","\n","# Create target directories in Colab if they don't exist\n","os.makedirs(COLAB_POS_TRAIN_DATA_DIR, exist_ok=True)\n","os.makedirs(COLAB_POS_RECON_BVH_DIR, exist_ok=True)\n","\n","print(f\"Attempting to copy from {GDRIVE_POS_TRAIN_DATA_SOURCE_DIR} to {COLAB_POS_TRAIN_DATA_DIR}...\")\n","if os.path.exists(GDRIVE_POS_TRAIN_DATA_SOURCE_DIR) and os.listdir(GDRIVE_POS_TRAIN_DATA_SOURCE_DIR):\n","  get_ipython().system(f\"cp -r {GDRIVE_POS_TRAIN_DATA_SOURCE_DIR}/* {COLAB_POS_TRAIN_DATA_DIR}/\")\n","  print(f\"Copying training data from Drive complete. Check: !ls {COLAB_POS_TRAIN_DATA_DIR}\")\n","else:\n","  print(f\"Source directory {GDRIVE_POS_TRAIN_DATA_SOURCE_DIR} is empty or does not exist in Google Drive. Skipping copy.\")\n","\n","print(f\"Attempting to copy from {GDRIVE_POS_RECON_BVH_SOURCE_DIR} to {COLAB_POS_RECON_BVH_DIR}...\")\n","if os.path.exists(GDRIVE_POS_RECON_BVH_SOURCE_DIR) and os.listdir(GDRIVE_POS_RECON_BVH_SOURCE_DIR):\n","  get_ipython().system(f\"cp -r {GDRIVE_POS_RECON_BVH_SOURCE_DIR}/* {COLAB_POS_RECON_BVH_DIR}/\")\n","  print(f\"Copying reconstructed BVH data from Drive complete. Check: !ls {COLAB_POS_RECON_BVH_DIR}\")\n","else:\n","  print(f\"Source directory {GDRIVE_POS_RECON_BVH_SOURCE_DIR} is empty or does not exist in Google Drive. Skipping copy.\")\n","\n","print(\"Process to load positional data from Google Drive finished.\")\n","# You can verify by listing contents in Colab:\n","# !ls /content/project645/train_data_pos/salsa/\n","# !ls /content/project645/reconstructed_bvh_data_pos/salsa/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZZkV3TjZRTQ","executionInfo":{"status":"ok","timestamp":1747850076207,"user_tz":-180,"elapsed":27008,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"4d8c0bec-eb74-4922-f25a-4010440277b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to copy from /content/drive/MyDrive/mai645_project_outputs_analysis/train_data_pos/salsa to /content/project645/train_data_pos/salsa...\n","Copying training data from Drive complete. Check: !ls /content/project645/train_data_pos/salsa\n","Attempting to copy from /content/drive/MyDrive/mai645_project_outputs_analysis/reconstructed_bvh_data_pos/salsa to /content/project645/reconstructed_bvh_data_pos/salsa...\n","Source directory /content/drive/MyDrive/mai645_project_outputs_analysis/reconstructed_bvh_data_pos/salsa is empty or does not exist in Google Drive. Skipping copy.\n","Process to load positional data from Google Drive finished.\n"]}]},{"cell_type":"markdown","source":["## (19m) Preprocess Positional data"],"metadata":{"id":"xySwOJHZZd69"}},{"cell_type":"code","source":["# Preprocess Positional data\n","!python generate_training_pos_data.py\n","# Check if output folders were created for salsa\n","!ls /content/project645/train_data_pos/\n","!ls /content/project645/train_data_pos/salsa/\n","!ls /content/project645/reconstructed_bvh_data_pos/salsa/"],"metadata":{"id":"Iy2uBR-z6CTq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747849747468,"user_tz":-180,"elapsed":9953,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"78643141-ae90-4b41-a799-75ef720fb334"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","object address  : 0x7b7d771fbdc0\n","object refcount : 2\n","object type     : 0x9d5ea0\n","object type name: KeyboardInterrupt\n","object repr     : KeyboardInterrupt()\n","lost sys.stderr\n","^C\n","salsa\n"]}]},{"cell_type":"markdown","source":["## Save Positional Data to Drive & Download (Optional)"],"metadata":{"id":"J3WJNIoXRyzM"}},{"cell_type":"code","source":["# Ensure GDRIVE_OUTPUT_DIR is defined\n","# GDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/mai645_project_outputs\" # Uncomment and adjust if not run in a previous cell\n","\n","# Define source and target directories for positional data\n","COLAB_POS_TRAIN_DATA_DIR = \"/content/project645/train_data_pos/salsa\"\n","COLAB_POS_RECON_BVH_DIR = \"/content/project645/reconstructed_bvh_data_pos/salsa\"\n","\n","GDRIVE_POS_TRAIN_DATA_TARGET_DIR = os.path.join(GDRIVE_OUTPUT_DIR, \"train_data_pos\", \"salsa\")\n","GDRIVE_POS_RECON_BVH_TARGET_DIR = os.path.join(GDRIVE_OUTPUT_DIR, \"reconstructed_bvh_data_pos\", \"salsa\")\n","\n","# Create target directories in Google Drive\n","os.makedirs(GDRIVE_POS_TRAIN_DATA_TARGET_DIR, exist_ok=True)\n","os.makedirs(GDRIVE_POS_RECON_BVH_TARGET_DIR, exist_ok=True)\n","\n","print(f\"Copying {COLAB_POS_TRAIN_DATA_DIR} to {GDRIVE_POS_TRAIN_DATA_TARGET_DIR}...\")\n","# Use !cp for shell command execution in Colab\n","# Make sure the source directory is not empty before copying\n","# Adding a check here, or ensure your preprocessing script ran successfully\n","if os.path.exists(COLAB_POS_TRAIN_DATA_DIR) and os.listdir(COLAB_POS_TRAIN_DATA_DIR):\n","  get_ipython().system(f\"cp -r {COLAB_POS_TRAIN_DATA_DIR}/* {GDRIVE_POS_TRAIN_DATA_TARGET_DIR}/\")\n","  print(\"Copying training data complete.\")\n","else:\n","  print(f\"Source directory {COLAB_POS_TRAIN_DATA_DIR} is empty or does not exist. Skipping copy.\")\n","\n","print(f\"Copying {COLAB_POS_RECON_BVH_DIR} to {GDRIVE_POS_RECON_BVH_TARGET_DIR}...\")\n","if os.path.exists(COLAB_POS_RECON_BVH_DIR) and os.listdir(COLAB_POS_RECON_BVH_DIR):\n","  get_ipython().system(f\"cp -r {COLAB_POS_RECON_BVH_DIR}/* {GDRIVE_POS_RECON_BVH_TARGET_DIR}/\")\n","  print(\"Copying reconstructed BVH data complete.\")\n","else:\n","  print(f\"Source directory {COLAB_POS_RECON_BVH_DIR} is empty or does not exist. Skipping copy.\")\n","\n","# Zip the data for download\n","ZIP_POS_TRAIN_NAME = \"project645_train_data_pos_salsa.zip\"\n","ZIP_POS_RECON_NAME = \"project645_reconstructed_bvh_pos_salsa.zip\"\n","\n","print(f\"Zipping {COLAB_POS_TRAIN_DATA_DIR} to {ZIP_POS_TRAIN_NAME}...\")\n","if os.path.exists(COLAB_POS_TRAIN_DATA_DIR) and os.listdir(COLAB_POS_TRAIN_DATA_DIR):\n","    get_ipython().system(f\"zip -r -q /content/{ZIP_POS_TRAIN_NAME} {COLAB_POS_TRAIN_DATA_DIR}\")\n","    print(f\"Zipping training data complete. Ready for download: {ZIP_POS_TRAIN_NAME}\")\n","    try:\n","        files.download(f\"/content/{ZIP_POS_TRAIN_NAME}\")\n","    except NameError: # files might not be imported if this cell is run standalone\n","        from google.colab import files\n","        files.download(f\"/content/{ZIP_POS_TRAIN_NAME}\")\n","else:\n","    print(f\"Source directory {COLAB_POS_TRAIN_DATA_DIR} is empty or does not exist. Skipping zip and download.\")\n","\n","\n","print(f\"Zipping {COLAB_POS_RECON_BVH_DIR} to {ZIP_POS_RECON_NAME}...\")\n","if os.path.exists(COLAB_POS_RECON_BVH_DIR) and os.listdir(COLAB_POS_RECON_BVH_DIR):\n","    get_ipython().system(f\"zip -r -q /content/{ZIP_POS_RECON_NAME} {COLAB_POS_RECON_BVH_DIR}\")\n","    print(f\"Zipping reconstructed BVH data complete. Ready for download: {ZIP_POS_RECON_NAME}\")\n","    try:\n","        files.download(f\"/content/{ZIP_POS_RECON_NAME}\")\n","    except NameError:\n","        from google.colab import files\n","        files.download(f\"/content/{ZIP_POS_RECON_NAME}\")\n","else:\n","    print(f\"Source directory {COLAB_POS_RECON_BVH_DIR} is empty or does not exist. Skipping zip and download.\")\n","\n","print(\"Data saving and zipping process finished.\")\n","# You can verify by listing contents in Drive:\n","# !ls \"{GDRIVE_POS_TRAIN_DATA_TARGET_DIR}\"\n","# !ls \"{GDRIVE_POS_RECON_BVH_TARGET_DIR}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvY8mzYERwlo","executionInfo":{"status":"ok","timestamp":1747849686614,"user_tz":-180,"elapsed":70,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"d25221e7-515a-4837-e608-2e98d818c32e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying /content/project645/train_data_pos/salsa to /content/drive/MyDrive/mai645_project_outputs_analysis/train_data_pos/salsa...\n","Source directory /content/project645/train_data_pos/salsa is empty or does not exist. Skipping copy.\n","Copying /content/project645/reconstructed_bvh_data_pos/salsa to /content/drive/MyDrive/mai645_project_outputs_analysis/reconstructed_bvh_data_pos/salsa...\n","Source directory /content/project645/reconstructed_bvh_data_pos/salsa is empty or does not exist. Skipping copy.\n","Zipping /content/project645/train_data_pos/salsa to project645_train_data_pos_salsa.zip...\n","Source directory /content/project645/train_data_pos/salsa is empty or does not exist. Skipping zip and download.\n","Zipping /content/project645/reconstructed_bvh_data_pos/salsa to project645_reconstructed_bvh_pos_salsa.zip...\n","Source directory /content/project645/reconstructed_bvh_data_pos/salsa is empty or does not exist. Skipping zip and download.\n","Data saving and zipping process finished.\n"]}]},{"cell_type":"markdown","source":["# Euler Preprocessing"],"metadata":{"id":"Zcj2a5Jh7RCa"}},{"cell_type":"code","source":["# Preprocess Euler data\n","!python generate_training_euler_data.py\n","# Check if output folders were created\n","!ls /content/project645/train_data_euler/\n","!ls /content/project645/train_data_euler/salsa/\n","!ls /content/project645/reconstructed_bvh_data_euler/salsa/"],"metadata":{"id":"cgr-cC_57QtN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747848950357,"user_tz":-180,"elapsed":7448,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"550f6462-eaa2-452e-82d1-81b00bdec006"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Euler Data Generation (with Hip Position Normalization) ---\n","Source BVH: ../train_data_bvh/salsa\n","Target .npy (Normalized Euler): ../train_data_euler/salsa/\n","Target reconstructed BVH: ../reconstructed_bvh_data_euler/salsa/\n","Standard BVH for reconstruction: ../train_data_bvh/standard.bvh\n","Hip Position Scale Factor: 0.01\n","\n","Step 1: Converting BVH files to Normalized Euler representation (.npy)...\n","Created directory: ../train_data_euler/salsa/\n","Found 30 BVH files in ../train_data_bvh/salsa\n","Processing (encode): ../train_data_bvh/salsa/09.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/09.npy with shape (2576, 132)\n","Processing (encode): ../train_data_bvh/salsa/27.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/27.npy with shape (1691, 132)\n","Processing (encode): ../train_data_bvh/salsa/16.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/16.npy with shape (2243, 132)\n","Processing (encode): ../train_data_bvh/salsa/24.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/24.npy with shape (2576, 132)\n","Processing (encode): ../train_data_bvh/salsa/23.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/23.npy with shape (3422, 132)\n","Processing (encode): ../train_data_bvh/salsa/20.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/20.npy with shape (1679, 132)\n","Processing (encode): ../train_data_bvh/salsa/01.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/01.npy with shape (2243, 132)\n","Processing (encode): ../train_data_bvh/salsa/05.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/05.npy with shape (1679, 132)\n","Processing (encode): ../train_data_bvh/salsa/21.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/21.npy with shape (1773, 132)\n","Processing (encode): ../train_data_bvh/salsa/26.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/26.npy with shape (2109, 132)\n","Processing (encode): ../train_data_bvh/salsa/03.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/03.npy with shape (1831, 132)\n","Processing (encode): ../train_data_bvh/salsa/30.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/30.npy with shape (2141, 132)\n","Processing (encode): ../train_data_bvh/salsa/28.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/28.npy with shape (2272, 132)\n","Processing (encode): ../train_data_bvh/salsa/19.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/19.npy with shape (1872, 132)\n","Processing (encode): ../train_data_bvh/salsa/14.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/14.npy with shape (1950, 132)\n","Processing (encode): ../train_data_bvh/salsa/10.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/10.npy with shape (1198, 132)\n","Processing (encode): ../train_data_bvh/salsa/22.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/22.npy with shape (2072, 132)\n","Processing (encode): ../train_data_bvh/salsa/13.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/13.npy with shape (2272, 132)\n","Processing (encode): ../train_data_bvh/salsa/04.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/04.npy with shape (1872, 132)\n","Processing (encode): ../train_data_bvh/salsa/08.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/08.npy with shape (3422, 132)\n","Processing (encode): ../train_data_bvh/salsa/07.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/07.npy with shape (2072, 132)\n","Processing (encode): ../train_data_bvh/salsa/15.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/15.npy with shape (2141, 132)\n","Processing (encode): ../train_data_bvh/salsa/18.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/18.npy with shape (1831, 132)\n","Processing (encode): ../train_data_bvh/salsa/06.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/06.npy with shape (1773, 132)\n","Processing (encode): ../train_data_bvh/salsa/29.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/29.npy with shape (1950, 132)\n","Processing (encode): ../train_data_bvh/salsa/17.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/17.npy with shape (2102, 132)\n","Processing (encode): ../train_data_bvh/salsa/25.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/25.npy with shape (1198, 132)\n","Processing (encode): ../train_data_bvh/salsa/11.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/11.npy with shape (2109, 132)\n","Processing (encode): ../train_data_bvh/salsa/02.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/02.npy with shape (2102, 132)\n","Processing (encode): ../train_data_bvh/salsa/12.bvh\n","Saved normalized Euler data to ../train_data_euler/salsa/12.npy with shape (1691, 132)\n","\n","Step 2: Converting Normalized Euler .npy data back to BVH for verification (hip inverse-scaled)...\n","Created directory: ../reconstructed_bvh_data_euler/salsa/\n","Found 30 NPY files in ../train_data_euler/salsa/\n","Processing (decode): ../train_data_euler/salsa/08.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/08.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/06.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/06.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/28.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/28.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/16.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/16.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/29.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/29.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/26.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/26.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/11.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/11.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/25.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/25.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/23.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/23.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/01.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/01.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/21.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/21.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/17.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/17.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/27.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/27.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/03.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/03.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/22.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/22.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/12.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/12.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/10.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/10.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/20.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/20.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/14.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/14.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/05.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/05.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/18.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/18.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/15.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/15.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/09.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/09.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/19.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/19.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/02.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/02.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/07.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/07.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/13.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/13.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/04.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/04.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/30.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/30.bvh (hip positions inverse-scaled only)\n","Processing (decode): ../train_data_euler/salsa/24.npy\n","Saved reconstructed BVH file to ../reconstructed_bvh_data_euler/salsa/24.bvh (hip positions inverse-scaled only)\n","\n","Normalized Euler data processing completed.\n","salsa\n","01.npy\t04.npy\t07.npy\t10.npy\t13.npy\t16.npy\t19.npy\t22.npy\t25.npy\t28.npy\n","02.npy\t05.npy\t08.npy\t11.npy\t14.npy\t17.npy\t20.npy\t23.npy\t26.npy\t29.npy\n","03.npy\t06.npy\t09.npy\t12.npy\t15.npy\t18.npy\t21.npy\t24.npy\t27.npy\t30.npy\n","01.bvh\t04.bvh\t07.bvh\t10.bvh\t13.bvh\t16.bvh\t19.bvh\t22.bvh\t25.bvh\t28.bvh\n","02.bvh\t05.bvh\t08.bvh\t11.bvh\t14.bvh\t17.bvh\t20.bvh\t23.bvh\t26.bvh\t29.bvh\n","03.bvh\t06.bvh\t09.bvh\t12.bvh\t15.bvh\t18.bvh\t21.bvh\t24.bvh\t27.bvh\t30.bvh\n"]}]},{"cell_type":"markdown","source":["# Quaternion Preprocess"],"metadata":{"id":"GEHg6mh_7Rco"}},{"cell_type":"code","source":["# Define path for cached preprocessed data archive on Google Drive\n","PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH = os.path.join(GDRIVE_OUTPUT_DIR, \"preprocessed_quad_data.tar.gz\")\n","# Define a path to a key file that indicates preprocessing was successful and data is present\n","# This file is expected to be created by generate_training_quad_data.py\n","PREPROCESSED_QUAD_DONE_FLAG_PATH = \"/content/project645/train_data_quad/salsa/metadata_quad.json\"\n","\n","# Variable to track if actual preprocessing is needed or if cached data can be used\n","preprocessing_needed = True"],"metadata":{"id":"_ujTUHdUto1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load Cached Preprocessed Quaternion Data (if available)\n","\n","if os.path.exists(PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH):\n","  print(f\"Found cached preprocessed data archive at {PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH}.\")\n","  print(\"Attempting to load...\")\n","  # Ensure the base target directory for extraction exists\n","  !mkdir -p /content/project645\n","  # Copy from Drive to local Colab environment\n","  !cp \"{PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH}\" /tmp/preprocessed_quad_data.tar.gz\n","  # Extract the archive into /content/project645/\n","  # The archive was created with paths relative to /content/project645\n","  print(\"Extracting data...\")\n","  !tar -xzf /tmp/preprocessed_quad_data.tar.gz -C /content/project645/\n","\n","  # Verify if a key file (flag path) now exists after extraction\n","  if os.path.exists(PREPROCESSED_QUAD_DONE_FLAG_PATH):\n","    print(\"Successfully loaded and verified cached preprocessed data.\")\n","    preprocessing_needed = False\n","  else:\n","    print(f\"Cached data archive found and extracted, but key file {PREPROCESSED_QUAD_DONE_FLAG_PATH} is missing.\")\n","    print(\"This might indicate an issue with the cache or the extraction process. Will re-preprocess.\")\n","    # Attempt to clean up potentially incomplete/corrupted extraction to avoid issues\n","    !rm -rf /content/project645/train_data_quad\n","    !rm -rf /content/project645/reconstructed_bvh_data_quad\n","    preprocessing_needed = True # Ensure it's true if verification fails\n","else:\n","  print(f\"No cached preprocessed data archive found at {PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH}.\")\n","  print(\"Proceeding with fresh preprocessing.\")\n","  preprocessing_needed = True # Explicitly set, though it's the default\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDCfzNjqtt-F","executionInfo":{"status":"ok","timestamp":1747849209070,"user_tz":-180,"elapsed":3026,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"14b0daa7-6187-48fe-d82b-b3f86b815ecf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found cached preprocessed data archive at /content/drive/MyDrive/mai645_project_outputs_analysis/preprocessed_quad_data.tar.gz.\n","Attempting to load...\n","Extracting data...\n","Successfully loaded and verified cached preprocessed data.\n"]}]},{"cell_type":"code","source":["if preprocessing_needed:\n","  print(\"Running Quaternion preprocessing script as cached data was not used...\")\n","  # Preprocess Quaternion data\n","  !python generate_training_quad_data.py\n","\n","  # Verify that preprocessing script created the expected outputs and the flag file\n","  if os.path.exists(PREPROCESSED_QUAD_DONE_FLAG_PATH):\n","    print(\"Preprocessing script completed. Expected output flag file found.\")\n","    print(\"Checking output directories:\")\n","    !ls /content/project645/train_data_quad/\n","    !ls /content/project645/train_data_quad/salsa/\n","    !ls /content/project645/reconstructed_bvh_data_quad/salsa/\n","\n","    # After successful preprocessing, save the results to Google Drive\n","    print(\"\\nPreprocessing finished. Saving data to Google Drive for future runs...\")\n","    # Create the archive.\n","    # -C /content/project645 makes paths in archive relative (e.g., train_data_quad, reconstructed_bvh_data_quad)\n","    # The directories to archive are /content/project645/train_data_quad and /content/project645/reconstructed_bvh_data_quad\n","    !tar -czf /tmp/preprocessed_quad_data.tar.gz -C /content/project645 train_data_quad reconstructed_bvh_data_quad\n","    !cp /tmp/preprocessed_quad_data.tar.gz \"{PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH}\"\n","    print(f\"Preprocessed data successfully saved to {PREPROCESSED_QUAD_ARCHIVE_GDRIVE_PATH}\")\n","  else:\n","    print(f\"Preprocessing script ran, but the key output flag file ({PREPROCESSED_QUAD_DONE_FLAG_PATH}) was not found.\")\n","    print(\"Data will not be cached. Please check the preprocessing script and its outputs.\")\n","    print(\"Listing contents of relevant directories for debugging (if they exist):\")\n","    !ls -ld /content/project645/train_data_quad/ # Use -ld to show directory info or error\n","    !ls -ld /content/project645/train_data_quad/salsa/\n","    !ls -ld /content/project645/reconstructed_bvh_data_quad/\n","    !ls -ld /content/project645/reconstructed_bvh_data_quad/salsa/\n","\n","else:\n","  print(\"Skipping Quaternion preprocessing as cached data was successfully loaded.\")\n","  print(\"Verifying existence of loaded data directories:\")\n","  # Confirm the directories expected from cache are present\n","  !ls /content/project645/train_data_quad/\n","  !ls /content/project645/train_data_quad/salsa/\n","  !ls /content/project645/reconstructed_bvh_data_quad/salsa/\n"],"metadata":{"id":"qMihsAXa7RjS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747849211618,"user_tz":-180,"elapsed":408,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"2a214741-e358-4486-d54d-0d71822255ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping Quaternion preprocessing as cached data was successfully loaded.\n","Verifying existence of loaded data directories:\n","salsa\n","01.npy\t05.npy\t09.npy\t13.npy\t17.npy\t21.npy\t25.npy\t29.npy\n","02.npy\t06.npy\t10.npy\t14.npy\t18.npy\t22.npy\t26.npy\t30.npy\n","03.npy\t07.npy\t11.npy\t15.npy\t19.npy\t23.npy\t27.npy\tmetadata_quad.json\n","04.npy\t08.npy\t12.npy\t16.npy\t20.npy\t24.npy\t28.npy\n","01.bvh\t04.bvh\t07.bvh\t10.bvh\t13.bvh\t16.bvh\t19.bvh\t22.bvh\t25.bvh\t28.bvh\n","02.bvh\t05.bvh\t08.bvh\t11.bvh\t14.bvh\t17.bvh\t20.bvh\t23.bvh\t26.bvh\t29.bvh\n","03.bvh\t06.bvh\t09.bvh\t12.bvh\t15.bvh\t18.bvh\t21.bvh\t24.bvh\t27.bvh\t30.bvh\n"]}]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"PHzsDI2kjb_Y"}},{"cell_type":"code","source":["\n","# Ensure you are in the correct directory where analysis.py is located\n","# This should be /content/project645/code/\n","import os\n","# Verify current directory, should already be /content/project645/code/ from previous cells\n","print(f\"Current directory for analysis: {os.getcwd()}\")\n","\n","# Before running analysis.py, make sure that:\n","# 1. The project645.zip has been uploaded and unzipped.\n","# 2. Google Drive is mounted.\n","# 3. The GDRIVE_OUTPUT_DIR variable is correctly set (e.g., mai645_project_outputs_con5 for Quad Con5 analysis).\n","#    The analysis.py script expects specific weight files to be present in paths constructed using this GDRIVE_OUTPUT_DIR.\n","#    For example, if GDRIVE_OUTPUT_DIR points to mai645_project_outputs_con5, then\n","#    QUAD_CON5_WEIGHT_PATH in analysis.py becomes \"/content/drive/MyDrive/mai645_project_outputs_con5/weights_quad/0028000.weight\".\n","#    Ensure these weight files (0049000.weight for Pos, 0051000.weight for Euler, 0028000.weight for Quad_Con5 etc.)\n","#    actually exist in your Google Drive output folders.\n","#    You might need to update the placeholder weight file names in analysis.py for QUAD_CON30 and QUAD_CON45.\n","\n","# Also, ensure that the preprocessed data required by analysis.py (train_data_pos/salsa, etc.)\n","# exists in /content/project645/. This data is typically generated by the preprocessing steps\n","# earlier in this Colab notebook or loaded from a cache. If you skipped those, analysis.py might fail.\n","\n","# List the contents of the code directory to confirm analysis.py is there\n","print(\"\\nListing contents of /content/project645/code/:\")\n","get_ipython().system('ls /content/project645/code/')\n","\n","print(\"\\nRunning analysis.py script...\")\n","# The analysis.py script is expected to be in /content/project645/code/\n","# It will use relative paths for data (e.g., ../train_data_pos)\n","# and absolute paths for weights (e.g., /content/drive/MyDrive/mai645_project_outputs/weights_pos/...)\n","get_ipython().system('python analysis.py')\n","\n","print(\"\\n--- Analysis script execution finished. Check the output above for results and plots. ---\")\n","print(f\"Plots will be saved in /content/project645/results_quantitative_partB/ if the script ran correctly.\")\n","print(\"You can also copy them to your Google Drive if needed, e.g.:\")"],"metadata":{"id":"m4I1ju6cjdsv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747850704684,"user_tz":-180,"elapsed":230004,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"237ec057-dd2c-4102-b21d-aa681d9d68d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current directory for analysis: /content/project645/code\n","\n","Listing contents of /content/project645/code/:\n","analysis.py\t\t\t read_bvh.pyc\n","fix_feet.py\t\t\t rotation2xyz.py\n","generate_training_euler_data.py  rotation2xyz.pyc\n","generate_training_pos_data.py\t rotation_conversions.py\n","generate_training_quad_data.py\t synthesize_euler_motion.py\n","__pycache__\t\t\t synthesize_pos_motion_original_colab.py\n","pytorch_train_euler_aclstm.py\t synthesize_pos_motion.py\n","pytorch_train_pos_aclstm.py\t synthesize_quad_motion_debug.py\n","pytorch_train_quad_aclstm.py\t synthesize_quad_motion.py\n","read_bvh_hierarchy.py\t\t temp_synthesis_output_partB\n","read_bvh_hierarchy.pyc\t\t test_encodings.py\n","read_bvh.py\n","\n","Running analysis.py script...\n","\n","--- Running Experiment: Positional ---\n","  Testing with initial_seq_len: 100\n","    Run 1/3 for seq_len 100...\n","      Run 1 metric: 0.0586\n","    Run 2/3 for seq_len 100...\n","      Run 2 metric: 0.0592\n","    Run 3/3 for seq_len 100...\n","      Run 3 metric: 0.0533\n","  Avg error for initial_seq_len 100: 0.0570 (+/- 0.0026) from 3 runs.\n","  Testing with initial_seq_len: 200\n","    Run 1/3 for seq_len 200...\n","      Run 1 metric: 0.0166\n","    Run 2/3 for seq_len 200...\n","      Run 2 metric: 0.0463\n","    Run 3/3 for seq_len 200...\n","      Run 3 metric: 0.0571\n","  Avg error for initial_seq_len 200: 0.0400 (+/- 0.0171) from 3 runs.\n","  Testing with initial_seq_len: 300\n","    Run 1/3 for seq_len 300...\n","      Run 1 metric: 0.1579\n","    Run 2/3 for seq_len 300...\n","      Run 2 metric: 0.1510\n","    Run 3/3 for seq_len 300...\n","      Run 3 metric: 0.0298\n","  Avg error for initial_seq_len 300: 0.1129 (+/- 0.0588) from 3 runs.\n","\n","--- Running Experiment: Euler ---\n","  Testing with initial_seq_len: 100\n","    Run 1/3 for seq_len 100...\n","      Run 1 metric: 0.0011\n","    Run 2/3 for seq_len 100...\n","      Run 2 metric: 0.0007\n","    Run 3/3 for seq_len 100...\n","      Run 3 metric: 0.0012\n","  Avg error for initial_seq_len 100: 0.0010 (+/- 0.0002) from 3 runs.\n","  Testing with initial_seq_len: 200\n","    Run 1/3 for seq_len 200...\n","      Run 1 metric: 0.0003\n","    Run 2/3 for seq_len 200...\n","      Run 2 metric: 0.0001\n","    Run 3/3 for seq_len 200...\n","      Run 3 metric: 0.0051\n","  Avg error for initial_seq_len 200: 0.0018 (+/- 0.0023) from 3 runs.\n","  Testing with initial_seq_len: 300\n","    Run 1/3 for seq_len 300...\n","      Run 1 metric: 0.0007\n","    Run 2/3 for seq_len 300...\n","      Run 2 metric: 0.0011\n","    Run 3/3 for seq_len 300...\n","      Run 3 metric: 0.0004\n","  Avg error for initial_seq_len 300: 0.0007 (+/- 0.0003) from 3 runs.\n","\n","--- Running Experiment: Quaternion (Con5) ---\n","  Testing with initial_seq_len: 100\n","    Run 1/3 for seq_len 100...\n","      Run 1 metric: 0.1057\n","    Run 2/3 for seq_len 100...\n","      Run 2 metric: 0.0910\n","    Run 3/3 for seq_len 100...\n","      Run 3 metric: 0.0624\n","  Avg error for initial_seq_len 100: 0.0864 (+/- 0.0180) from 3 runs.\n","  Testing with initial_seq_len: 200\n","    Run 1/3 for seq_len 200...\n","      Run 1 metric: 0.1143\n","    Run 2/3 for seq_len 200...\n","      Run 2 metric: 0.0767\n","    Run 3/3 for seq_len 200...\n","      Run 3 metric: 0.0783\n","  Avg error for initial_seq_len 200: 0.0898 (+/- 0.0174) from 3 runs.\n","  Testing with initial_seq_len: 300\n","    Run 1/3 for seq_len 300...\n","      Run 1 metric: 0.0710\n","    Run 2/3 for seq_len 300...\n","      Run 2 metric: 0.1340\n","    Run 3/3 for seq_len 300...\n","      Run 3 metric: 0.0992\n","  Avg error for initial_seq_len 300: 0.1014 (+/- 0.0258) from 3 runs.\n","\n","--- Running Experiment: Quaternion (Con30) ---\n","  Testing with initial_seq_len: 100\n","    Run 1/3 for seq_len 100...\n","      Run 1 metric: 0.0102\n","    Run 2/3 for seq_len 100...\n","      Run 2 metric: 0.0848\n","    Run 3/3 for seq_len 100...\n","      Run 3 metric: 0.0356\n","  Avg error for initial_seq_len 100: 0.0435 (+/- 0.0310) from 3 runs.\n","  Testing with initial_seq_len: 200\n","    Run 1/3 for seq_len 200...\n","      Run 1 metric: 0.0438\n","    Run 2/3 for seq_len 200...\n","      Run 2 metric: 0.0158\n","    Run 3/3 for seq_len 200...\n","      Run 3 metric: 0.0196\n","  Avg error for initial_seq_len 200: 0.0264 (+/- 0.0124) from 3 runs.\n","  Testing with initial_seq_len: 300\n","    Run 1/3 for seq_len 300...\n","      Run 1 metric: 0.0407\n","    Run 2/3 for seq_len 300...\n","      Run 2 metric: 0.0274\n","    Run 3/3 for seq_len 300...\n","      Run 3 metric: 0.0638\n","  Avg error for initial_seq_len 300: 0.0440 (+/- 0.0150) from 3 runs.\n","\n","--- Running Experiment: Quaternion (Con45) ---\n","  Testing with initial_seq_len: 100\n","    Run 1/3 for seq_len 100...\n","      Run 1 metric: 0.0132\n","    Run 2/3 for seq_len 100...\n","      Run 2 metric: 0.0235\n","    Run 3/3 for seq_len 100...\n","      Run 3 metric: 0.0305\n","  Avg error for initial_seq_len 100: 0.0224 (+/- 0.0071) from 3 runs.\n","  Testing with initial_seq_len: 200\n","    Run 1/3 for seq_len 200...\n","      Run 1 metric: 0.0614\n","    Run 2/3 for seq_len 200...\n","      Run 2 metric: 0.0190\n","    Run 3/3 for seq_len 200...\n","      Run 3 metric: 0.0272\n","  Avg error for initial_seq_len 200: 0.0359 (+/- 0.0184) from 3 runs.\n","  Testing with initial_seq_len: 300\n","    Run 1/3 for seq_len 300...\n","      Run 1 metric: 0.0198\n","    Run 2/3 for seq_len 300...\n","      Run 2 metric: 0.0353\n","    Run 3/3 for seq_len 300...\n","      Run 3 metric: 0.0678\n","  Avg error for initial_seq_len 300: 0.0410 (+/- 0.0200) from 3 runs.\n","\n","Plot saved to /content/project645/results_quantitative_partB/quantitative_comparison_plot.png\n","Figure(1400x2100)\n","Cleaned up temporary directory: /content/project645/code/temp_synthesis_output_partB\n","\n","Experiment Script Finished.\n","\n","--- Analysis script execution finished. Check the output above for results and plots. ---\n","Plots will be saved in /content/project645/results_quantitative_partB/ if the script ran correctly.\n","You can also copy them to your Google Drive if needed, e.g.:\n"]}]},{"cell_type":"markdown","source":["# Plot loss positional and quad"],"metadata":{"id":"Z4b3jLZAGxLM"}},{"cell_type":"markdown","source":["to run this you need to use the file pytorch_train_pos_aclstm_for_calculating_loss_to_device by renaming it to pytorch_train_pos_aclstm. to handle a device error. but did not change the architecture or anything else. this is just to avoid changing the original file used for training."],"metadata":{"id":"9rTLp6_TNw_O"}},{"cell_type":"code","source":["import os\n","\n","\n","# Directory to save the output plots in Colab environment\n","COLAB_PLOT_OUTPUT_DIR = \"/content/project645/results_plot_loss\"\n","os.makedirs(COLAB_PLOT_OUTPUT_DIR, exist_ok=True)\n","\n","# Define paths to data (assuming preprocessing has been run)\n","POS_DATA_DIR = \"/content/project645/train_data_pos/salsa/\"\n","QUAD_DATA_DIR = \"/content/project645/train_data_quad/salsa/\"\n","METADATA_QUAD_PATH = os.path.join(QUAD_DATA_DIR, \"metadata_quad.json\")\n","\n","# Define paths to weight folders.\n","# IMPORTANT: Ensure these directories exist in your Google Drive and contain the .weight files from respective training runs.\n","POS_WEIGHTS_DIR = \"/content/drive/MyDrive/mai645_project_outputs/weights_pos\"\n","QUAD_WEIGHTS_CON5_DIR = \"/content/drive/MyDrive/mai645_project_outputs_con5/weights_quad\"\n","QUAD_WEIGHTS_CON30_DIR = \"/content/drive/MyDrive/mai645_project_outputs_con30/weights_quad\"\n","QUAD_WEIGHTS_CON45_DIR = \"/content/drive/MyDrive/mai645_project_outputs_con45/weights_quad\"\n","\n","# Check if the essential data and metadata paths exist before running\n","if not os.path.exists(POS_DATA_DIR):\n","    print(f\"ERROR: POS data folder not found: {POS_DATA_DIR}\")\n","    print(\"Please ensure POS preprocessing has been completed.\")\n","if not os.path.exists(QUAD_DATA_DIR):\n","    print(f\"ERROR: QUAD data folder not found: {QUAD_DATA_DIR}\")\n","    print(\"Please ensure QUAD preprocessing has been completed.\")\n","if not os.path.exists(METADATA_QUAD_PATH):\n","    print(f\"ERROR: QUAD metadata file not found: {METADATA_QUAD_PATH}\")\n","    print(\"Please ensure QUAD preprocessing has been completed.\")\n","\n","# Construct the command to run plot_loss.py\n","# Note: plot_loss.py is expected to be in /content/project645/code/\n","plot_script_path = \"/content/project645/code/plot_loss.py\"\n","\n","# Output file paths for plots\n","pos_plot_output = os.path.join(COLAB_PLOT_OUTPUT_DIR, \"pos_model_loss.png\")\n","quad_plot_output_prefix = os.path.join(COLAB_PLOT_OUTPUT_DIR, \"quad_model_loss\") # _{config_label}.png will be appended\n","\n","command = f\"\"\"\n","python {plot_script_path} \\\\\n","    --model_seq_len 100 \\\\\n","    --device cpu \\\\\n","    --pos_data_folder \"{POS_DATA_DIR}\" \\\\\n","    --pos_weights_folder \"{POS_WEIGHTS_DIR}\" \\\\\n","    --pos_output_plot \"{pos_plot_output}\" \\\\\n","    --quad_data_folder \"{QUAD_DATA_DIR}\" \\\\\n","    --quad_metadata_path \"{METADATA_QUAD_PATH}\" \\\\\n","    --quad_weights_folder_con5 \"{QUAD_WEIGHTS_CON5_DIR}\" \\\\\n","    --quad_weights_folder_con30 \"{QUAD_WEIGHTS_CON30_DIR}\" \\\\\n","    --quad_weights_folder_con45 \"{QUAD_WEIGHTS_CON45_DIR}\" \\\\\n","    --quad_output_plot_prefix \"{quad_plot_output_prefix}\"\n","\"\"\"\n","\n","print(\"Running plot_loss.py script...\")\n","print(\"This might take a few minutes depending on the number of weight files.\")\n","print(\"\\\\nCommand being executed:\")\n","print(command)\n","\n","# Execute the command\n","get_ipython().system(command)\n","\n","print(f\"\\\\n--- Loss plotting script finished. ---\")\n","print(f\"Plots should be saved in: {COLAB_PLOT_OUTPUT_DIR}\")\n","print(\"You can view them by navigating to the folder in the Colab file browser.\")\n","print(f\"To copy them to your Google Drive (e.g., into {GDRIVE_OUTPUT_DIR}/plots/):\")\n","# print(f'!mkdir -p \"{os.path.join(GDRIVE_OUTPUT_DIR, 'plots')}\"')\n","# print(f'!cp -r {COLAB_PLOT_OUTPUT_DIR}/* \"{os.path.join(GDRIVE_OUTPUT_DIR, 'plots/')}\"')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZZBai7qHL87","executionInfo":{"status":"ok","timestamp":1747854692337,"user_tz":-180,"elapsed":926368,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"8092e8ae-f180-469f-f327-89eb4b252d8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running plot_loss.py script...\n","This might take a few minutes depending on the number of weight files.\n","\\nCommand being executed:\n","\n","python /content/project645/code/plot_loss.py \\\n","    --model_seq_len 100 \\\n","    --device cpu \\\n","    --pos_data_folder \"/content/project645/train_data_pos/salsa/\" \\\n","    --pos_weights_folder \"/content/drive/MyDrive/mai645_project_outputs/weights_pos\" \\\n","    --pos_output_plot \"/content/project645/results_plot_loss/pos_model_loss.png\" \\\n","    --quad_data_folder \"/content/project645/train_data_quad/salsa/\" \\\n","    --quad_metadata_path \"/content/project645/train_data_quad/salsa/metadata_quad.json\" \\\n","    --quad_weights_folder_con5 \"/content/drive/MyDrive/mai645_project_outputs_con5/weights_quad\" \\\n","    --quad_weights_folder_con30 \"/content/drive/MyDrive/mai645_project_outputs_con30/weights_quad\" \\\n","    --quad_weights_folder_con45 \"/content/drive/MyDrive/mai645_project_outputs_con45/weights_quad\" \\\n","    --quad_output_plot_prefix \"/content/project645/results_plot_loss/quad_model_loss\"\n","\n","Plotting POS model loss from weights in: /content/drive/MyDrive/mai645_project_outputs/weights_pos\n","Loading motion files...\n","24  Motion files loaded\n","Processing POS weight: 0000000.weight (Iteration: 0)\n","Processing POS weight: 0001000.weight (Iteration: 1000)\n","Processing POS weight: 0002000.weight (Iteration: 2000)\n","Processing POS weight: 0003000.weight (Iteration: 3000)\n","Processing POS weight: 0004000.weight (Iteration: 4000)\n","Processing POS weight: 0005000.weight (Iteration: 5000)\n","Processing POS weight: 0006000.weight (Iteration: 6000)\n","Processing POS weight: 0007000.weight (Iteration: 7000)\n","Processing POS weight: 0008000.weight (Iteration: 8000)\n","Processing POS weight: 0009000.weight (Iteration: 9000)\n","Processing POS weight: 0010000.weight (Iteration: 10000)\n","Processing POS weight: 0011000.weight (Iteration: 11000)\n","Processing POS weight: 0012000.weight (Iteration: 12000)\n","Processing POS weight: 0013000.weight (Iteration: 13000)\n","Processing POS weight: 0014000.weight (Iteration: 14000)\n","Processing POS weight: 0015000.weight (Iteration: 15000)\n","Processing POS weight: 0016000.weight (Iteration: 16000)\n","Processing POS weight: 0017000.weight (Iteration: 17000)\n","Processing POS weight: 0018000.weight (Iteration: 18000)\n","Processing POS weight: 0019000.weight (Iteration: 19000)\n","Processing POS weight: 0020000.weight (Iteration: 20000)\n","Processing POS weight: 0021000.weight (Iteration: 21000)\n","Processing POS weight: 0022000.weight (Iteration: 22000)\n","Processing POS weight: 0023000.weight (Iteration: 23000)\n","Processing POS weight: 0024000.weight (Iteration: 24000)\n","Processing POS weight: 0025000.weight (Iteration: 25000)\n","Processing POS weight: 0026000.weight (Iteration: 26000)\n","Processing POS weight: 0027000.weight (Iteration: 27000)\n","Processing POS weight: 0028000.weight (Iteration: 28000)\n","Processing POS weight: 0029000.weight (Iteration: 29000)\n","Processing POS weight: 0030000.weight (Iteration: 30000)\n","Processing POS weight: 0031000.weight (Iteration: 31000)\n","Processing POS weight: 0032000.weight (Iteration: 32000)\n","Processing POS weight: 0033000.weight (Iteration: 33000)\n","Processing POS weight: 0034000.weight (Iteration: 34000)\n","Processing POS weight: 0035000.weight (Iteration: 35000)\n","Processing POS weight: 0036000.weight (Iteration: 36000)\n","Processing POS weight: 0037000.weight (Iteration: 37000)\n","Processing POS weight: 0038000.weight (Iteration: 38000)\n","Processing POS weight: 0039000.weight (Iteration: 39000)\n","Processing POS weight: 0040000.weight (Iteration: 40000)\n","Processing POS weight: 0041000.weight (Iteration: 41000)\n","Processing POS weight: 0042000.weight (Iteration: 42000)\n","Processing POS weight: 0043000.weight (Iteration: 43000)\n","Processing POS weight: 0044000.weight (Iteration: 44000)\n","Processing POS weight: 0045000.weight (Iteration: 45000)\n","Processing POS weight: 0046000.weight (Iteration: 46000)\n","Processing POS weight: 0047000.weight (Iteration: 47000)\n","Processing POS weight: 0048000.weight (Iteration: 48000)\n","Processing POS weight: 0049000.weight (Iteration: 49000)\n","POS loss plot saved to /content/project645/results_plot_loss/pos_model_loss.png\n","Loaded metadata from /content/project645/train_data_quad/salsa/metadata_quad.json\n","Loading motion files from /content/project645/train_data_quad/salsa/...\n","30 motion files loaded.\n","\n","Processing QUAD model configuration: con5 from weights in: /content/drive/MyDrive/mai645_project_outputs_con5/weights_quad\n","Processing QUAD weight: 0001000.weight (Iteration: 1000) for con5\n","Processing QUAD weight: 0002000.weight (Iteration: 2000) for con5\n","Processing QUAD weight: 0003000.weight (Iteration: 3000) for con5\n","Processing QUAD weight: 0004000.weight (Iteration: 4000) for con5\n","Processing QUAD weight: 0005000.weight (Iteration: 5000) for con5\n","Processing QUAD weight: 0006000.weight (Iteration: 6000) for con5\n","Processing QUAD weight: 0007000.weight (Iteration: 7000) for con5\n","Processing QUAD weight: 0008000.weight (Iteration: 8000) for con5\n","Processing QUAD weight: 0009000.weight (Iteration: 9000) for con5\n","Processing QUAD weight: 0010000.weight (Iteration: 10000) for con5\n","Processing QUAD weight: 0011000.weight (Iteration: 11000) for con5\n","Processing QUAD weight: 0012000.weight (Iteration: 12000) for con5\n","Processing QUAD weight: 0013000.weight (Iteration: 13000) for con5\n","Processing QUAD weight: 0014000.weight (Iteration: 14000) for con5\n","Processing QUAD weight: 0015000.weight (Iteration: 15000) for con5\n","Processing QUAD weight: 0016000.weight (Iteration: 16000) for con5\n","Processing QUAD weight: 0017000.weight (Iteration: 17000) for con5\n","Processing QUAD weight: 0018000.weight (Iteration: 18000) for con5\n","Processing QUAD weight: 0019000.weight (Iteration: 19000) for con5\n","Processing QUAD weight: 0020000.weight (Iteration: 20000) for con5\n","Processing QUAD weight: 0021000.weight (Iteration: 21000) for con5\n","Processing QUAD weight: 0022000.weight (Iteration: 22000) for con5\n","Processing QUAD weight: 0023000.weight (Iteration: 23000) for con5\n","Processing QUAD weight: 0024000.weight (Iteration: 24000) for con5\n","Processing QUAD weight: 0025000.weight (Iteration: 25000) for con5\n","Processing QUAD weight: 0026000.weight (Iteration: 26000) for con5\n","Processing QUAD weight: 0027000.weight (Iteration: 27000) for con5\n","Processing QUAD weight: 0028000.weight (Iteration: 28000) for con5\n","QUAD loss plot for con5 saved to /content/project645/results_plot_loss/quad_model_loss_con5.png\n","\n","Processing QUAD model configuration: con30 from weights in: /content/drive/MyDrive/mai645_project_outputs_con30/weights_quad\n","Processing QUAD weight: 0001000.weight (Iteration: 1000) for con30\n","Processing QUAD weight: 0002000.weight (Iteration: 2000) for con30\n","Processing QUAD weight: 0003000.weight (Iteration: 3000) for con30\n","Processing QUAD weight: 0004000.weight (Iteration: 4000) for con30\n","Processing QUAD weight: 0005000.weight (Iteration: 5000) for con30\n","Processing QUAD weight: 0006000.weight (Iteration: 6000) for con30\n","Processing QUAD weight: 0007000.weight (Iteration: 7000) for con30\n","Processing QUAD weight: 0008000.weight (Iteration: 8000) for con30\n","Processing QUAD weight: 0009000.weight (Iteration: 9000) for con30\n","Processing QUAD weight: 0010000.weight (Iteration: 10000) for con30\n","Processing QUAD weight: 0011000.weight (Iteration: 11000) for con30\n","Processing QUAD weight: 0012000.weight (Iteration: 12000) for con30\n","Processing QUAD weight: 0013000.weight (Iteration: 13000) for con30\n","Processing QUAD weight: 0014000.weight (Iteration: 14000) for con30\n","Processing QUAD weight: 0015000.weight (Iteration: 15000) for con30\n","Processing QUAD weight: 0016000.weight (Iteration: 16000) for con30\n","Processing QUAD weight: 0017000.weight (Iteration: 17000) for con30\n","Processing QUAD weight: 0018000.weight (Iteration: 18000) for con30\n","Processing QUAD weight: 0019000.weight (Iteration: 19000) for con30\n","Processing QUAD weight: 0020000.weight (Iteration: 20000) for con30\n","Processing QUAD weight: 0021000.weight (Iteration: 21000) for con30\n","Processing QUAD weight: 0022000.weight (Iteration: 22000) for con30\n","Processing QUAD weight: 0023000.weight (Iteration: 23000) for con30\n","Processing QUAD weight: 0024000.weight (Iteration: 24000) for con30\n","Processing QUAD weight: 0025000.weight (Iteration: 25000) for con30\n","Processing QUAD weight: 0026000.weight (Iteration: 26000) for con30\n","Processing QUAD weight: 0027000.weight (Iteration: 27000) for con30\n","Processing QUAD weight: 0028000.weight (Iteration: 28000) for con30\n","Processing QUAD weight: 0029000.weight (Iteration: 29000) for con30\n","Processing QUAD weight: 0030000.weight (Iteration: 30000) for con30\n","Processing QUAD weight: 0031000.weight (Iteration: 31000) for con30\n","QUAD loss plot for con30 saved to /content/project645/results_plot_loss/quad_model_loss_con30.png\n","\n","Processing QUAD model configuration: con45 from weights in: /content/drive/MyDrive/mai645_project_outputs_con45/weights_quad\n","Processing QUAD weight: 0001000.weight (Iteration: 1000) for con45\n","Processing QUAD weight: 0002000.weight (Iteration: 2000) for con45\n","Processing QUAD weight: 0003000.weight (Iteration: 3000) for con45\n","Processing QUAD weight: 0004000.weight (Iteration: 4000) for con45\n","Processing QUAD weight: 0005000.weight (Iteration: 5000) for con45\n","Processing QUAD weight: 0006000.weight (Iteration: 6000) for con45\n","Processing QUAD weight: 0007000.weight (Iteration: 7000) for con45\n","Processing QUAD weight: 0008000.weight (Iteration: 8000) for con45\n","Processing QUAD weight: 0009000.weight (Iteration: 9000) for con45\n","Processing QUAD weight: 0010000.weight (Iteration: 10000) for con45\n","Processing QUAD weight: 0011000.weight (Iteration: 11000) for con45\n","Processing QUAD weight: 0012000.weight (Iteration: 12000) for con45\n","Processing QUAD weight: 0013000.weight (Iteration: 13000) for con45\n","Processing QUAD weight: 0014000.weight (Iteration: 14000) for con45\n","Processing QUAD weight: 0015000.weight (Iteration: 15000) for con45\n","Processing QUAD weight: 0016000.weight (Iteration: 16000) for con45\n","Processing QUAD weight: 0017000.weight (Iteration: 17000) for con45\n","Processing QUAD weight: 0018000.weight (Iteration: 18000) for con45\n","Processing QUAD weight: 0019000.weight (Iteration: 19000) for con45\n","Processing QUAD weight: 0020000.weight (Iteration: 20000) for con45\n","Processing QUAD weight: 0021000.weight (Iteration: 21000) for con45\n","Processing QUAD weight: 0022000.weight (Iteration: 22000) for con45\n","Processing QUAD weight: 0023000.weight (Iteration: 23000) for con45\n","Processing QUAD weight: 0024000.weight (Iteration: 24000) for con45\n","Processing QUAD weight: 0025000.weight (Iteration: 25000) for con45\n","Processing QUAD weight: 0026000.weight (Iteration: 26000) for con45\n","Processing QUAD weight: 0027000.weight (Iteration: 27000) for con45\n","Processing QUAD weight: 0028000.weight (Iteration: 28000) for con45\n","Processing QUAD weight: 0029000.weight (Iteration: 29000) for con45\n","Processing QUAD weight: 0030000.weight (Iteration: 30000) for con45\n","Processing QUAD weight: 0031000.weight (Iteration: 31000) for con45\n","Processing QUAD weight: 0032000.weight (Iteration: 32000) for con45\n","Processing QUAD weight: 0033000.weight (Iteration: 33000) for con45\n","Processing QUAD weight: 0034000.weight (Iteration: 34000) for con45\n","Processing QUAD weight: 0035000.weight (Iteration: 35000) for con45\n","Processing QUAD weight: 0036000.weight (Iteration: 36000) for con45\n","Processing QUAD weight: 0037000.weight (Iteration: 37000) for con45\n","Processing QUAD weight: 0038000.weight (Iteration: 38000) for con45\n","Processing QUAD weight: 0039000.weight (Iteration: 39000) for con45\n","Processing QUAD weight: 0040000.weight (Iteration: 40000) for con45\n","Processing QUAD weight: 0041000.weight (Iteration: 41000) for con45\n","Processing QUAD weight: 0042000.weight (Iteration: 42000) for con45\n","Processing QUAD weight: 0043000.weight (Iteration: 43000) for con45\n","Processing QUAD weight: 0044000.weight (Iteration: 44000) for con45\n","Processing QUAD weight: 0045000.weight (Iteration: 45000) for con45\n","Processing QUAD weight: 0046000.weight (Iteration: 46000) for con45\n","Processing QUAD weight: 0047000.weight (Iteration: 47000) for con45\n","Processing QUAD weight: 0048000.weight (Iteration: 48000) for con45\n","Processing QUAD weight: 0049000.weight (Iteration: 49000) for con45\n","QUAD loss plot for con45 saved to /content/project645/results_plot_loss/quad_model_loss_con45.png\n","\\n--- Loss plotting script finished. ---\n","Plots should be saved in: /content/project645/results_plot_loss\n","You can view them by navigating to the folder in the Colab file browser.\n","To copy them to your Google Drive (e.g., into /content/drive/MyDrive/mai645_project_outputs_analysis/plots/):\n"]}]},{"cell_type":"markdown","source":["# Plot loss euler"],"metadata":{"id":"Uc2ZMohGRfrf"}},{"cell_type":"markdown","source":["to run this you need to use the file pytorch_train_euler_aclstm_for_calculating_loss_to_device by renaming it to pytorch_train_euler_aclstm. to handle a device error. but did not change the architecture or anything else. this is just to avoid changing the original file used for training."],"metadata":{"id":"HTJv8gqMS_33"}},{"cell_type":"code","source":["import os\n","\n","# Directory to save the output plots in Colab environment (should be already created)\n","# COLAB_PLOT_OUTPUT_DIR = \"/content/project645/results_plot_loss\"\n","# os.makedirs(COLAB_PLOT_OUTPUT_DIR, exist_ok=True) # Already created in previous cell\n","\n","# Define paths to Euler data (assuming preprocessing has been run)\n","EULER_DATA_DIR = \"/content/project645/train_data_euler/salsa/\"\n","\n","# Define path to Euler weight folder.\n","# IMPORTANT: Ensure this directory exists in your Google Drive and contains the .weight files from the Euler training run.\n","# This path should align with what was used in project645_2euler.py for training.\n","# Common structure might be GDRIVE_OUTPUT_DIR (e.g., /content/drive/MyDrive/mai645_project_outputs) + /weights_euler\n","# Or if you have a separate mai645_project_outputs_analysis, ensure weights_euler is there or point to the original training output dir.\n","# For this example, let's assume it's in the general GDRIVE_OUTPUT_DIR from earlier cells if not overridden for analysis specifically.\n","# If GDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/mai645_project_outputs_analysis\", then EULER_WEIGHTS_DIR might be:\n","# EULER_WEIGHTS_DIR = os.path.join(GDRIVE_OUTPUT_DIR, \"weights_euler\")\n","# OR, if weights are in the original output folder:\n","EULER_WEIGHTS_DIR = \"/content/drive/MyDrive/mai645_project_outputs/weights_euler\" # Adjust if your structure is different\n","\n","# Check if the essential data and metadata paths exist before running\n","if not os.path.exists(EULER_DATA_DIR):\n","    print(f\"ERROR: Euler data folder not found: {EULER_DATA_DIR}\")\n","    print(\"Please ensure Euler preprocessing has been completed.\")\n","if not os.path.exists(EULER_WEIGHTS_DIR):\n","    print(f\"ERROR: Euler weights folder not found: {EULER_WEIGHTS_DIR}\")\n","    print(f\"Please ensure the folder exists and contains .weight files.\")\n","\n","# Construct the command to run plot_euler_loss.py\n","# Note: plot_euler_loss.py is expected to be in /content/project645/code/\n","euler_plot_script_path = \"/content/project645/code/plot_euler_loss.py\"\n","\n","# Output file path for the Euler plot (in the same folder as other plots)\n","euler_plot_output = os.path.join(COLAB_PLOT_OUTPUT_DIR, \"euler_model_loss.png\")\n","\n","# Frame size for Euler model is typically 132 as seen in project645_2euler.py\n","# It can also be auto-detected by the script if not provided.\n","euler_frame_channels = 132\n","\n","euler_command = f\"\"\"\n","python {euler_plot_script_path} \\\n","    --model_seq_len 100 \\\n","    --device cpu \\\n","    --euler_data_folder \"{EULER_DATA_DIR}\" \\\n","    --euler_weights_folder \"{EULER_WEIGHTS_DIR}\" \\\n","    --euler_output_plot \"{euler_plot_output}\" \\\n","    --euler_in_frame_size {euler_frame_channels} # Optional, can be auto-detected\n","\"\"\"\n","\n","print(\"Running plot_euler_loss.py script...\")\n","print(\"This might take a few minutes depending on the number of weight files.\")\n","print(\"\\nCommand being executed for Euler loss plot:\")\n","print(euler_command)\n","\n","# Execute the command\n","if os.path.exists(EULER_DATA_DIR) and os.path.exists(EULER_WEIGHTS_DIR):\n","    get_ipython().system(euler_command)\n","    print(f\"\\n--- Euler loss plotting script finished. --- \")\n","    print(f\"Plot should be saved in: {COLAB_PLOT_OUTPUT_DIR}\")\n","else:\n","    print(\"Skipping Euler loss plot generation due to missing data or weights folder.\")\n","\n","# You can view them by navigating to the folder in the Colab file browser.\n","# To copy to Google Drive (if COLAB_PLOT_OUTPUT_DIR and GDRIVE_OUTPUT_DIR are set):\n","# print(f'!mkdir -p \"{os.path.join(GDRIVE_OUTPUT_DIR, \"plots\")}' ) # Ensure plot dir exists on Drive\n","# print(f'!cp \"{euler_plot_output}\" \"{os.path.join(GDRIVE_OUTPUT_DIR, \"plots/\")}\"' ) if os.path.exists(euler_plot_output) else print(\"Euler plot not generated, skipping copy.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKfBLQzKRmvZ","executionInfo":{"status":"ok","timestamp":1747855494918,"user_tz":-180,"elapsed":300025,"user":{"displayName":"Chrysis Andreou","userId":"08304976464433857852"}},"outputId":"d0829e3e-09ba-433d-ffa4-22eb3f6cfdb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running plot_euler_loss.py script...\n","This might take a few minutes depending on the number of weight files.\n","\n","Command being executed for Euler loss plot:\n","\n","python /content/project645/code/plot_euler_loss.py     --model_seq_len 100     --device cpu     --euler_data_folder \"/content/project645/train_data_euler/salsa/\"     --euler_weights_folder \"/content/drive/MyDrive/mai645_project_outputs/weights_euler\"     --euler_output_plot \"/content/project645/results_plot_loss/euler_model_loss.png\"     --euler_in_frame_size 132 # Optional, can be auto-detected\n","\n","Plotting Euler model loss from weights in: /content/drive/MyDrive/mai645_project_outputs/weights_euler\n","Loading motion files from /content/project645/train_data_euler/salsa/...\n","30 motion files loaded.\n","Using provided Euler in_frame_size: 132\n","Processing Euler weight: 0001000.weight (Iteration: 1000)\n","Processing Euler weight: 0002000.weight (Iteration: 2000)\n","Processing Euler weight: 0003000.weight (Iteration: 3000)\n","Processing Euler weight: 0004000.weight (Iteration: 4000)\n","Processing Euler weight: 0005000.weight (Iteration: 5000)\n","Processing Euler weight: 0006000.weight (Iteration: 6000)\n","Processing Euler weight: 0007000.weight (Iteration: 7000)\n","Processing Euler weight: 0008000.weight (Iteration: 8000)\n","Processing Euler weight: 0009000.weight (Iteration: 9000)\n","Processing Euler weight: 0010000.weight (Iteration: 10000)\n","Processing Euler weight: 0011000.weight (Iteration: 11000)\n","Processing Euler weight: 0012000.weight (Iteration: 12000)\n","Processing Euler weight: 0013000.weight (Iteration: 13000)\n","Processing Euler weight: 0014000.weight (Iteration: 14000)\n","Processing Euler weight: 0015000.weight (Iteration: 15000)\n","Processing Euler weight: 0016000.weight (Iteration: 16000)\n","Processing Euler weight: 0017000.weight (Iteration: 17000)\n","Processing Euler weight: 0018000.weight (Iteration: 18000)\n","Processing Euler weight: 0019000.weight (Iteration: 19000)\n","Processing Euler weight: 0020000.weight (Iteration: 20000)\n","Processing Euler weight: 0021000.weight (Iteration: 21000)\n","Processing Euler weight: 0022000.weight (Iteration: 22000)\n","Processing Euler weight: 0023000.weight (Iteration: 23000)\n","Processing Euler weight: 0024000.weight (Iteration: 24000)\n","Processing Euler weight: 0025000.weight (Iteration: 25000)\n","Processing Euler weight: 0026000.weight (Iteration: 26000)\n","Processing Euler weight: 0027000.weight (Iteration: 27000)\n","Processing Euler weight: 0028000.weight (Iteration: 28000)\n","Processing Euler weight: 0029000.weight (Iteration: 29000)\n","Processing Euler weight: 0030000.weight (Iteration: 30000)\n","Processing Euler weight: 0031000.weight (Iteration: 31000)\n","Processing Euler weight: 0032000.weight (Iteration: 32000)\n","Processing Euler weight: 0033000.weight (Iteration: 33000)\n","Processing Euler weight: 0034000.weight (Iteration: 34000)\n","Processing Euler weight: 0035000.weight (Iteration: 35000)\n","Processing Euler weight: 0036000.weight (Iteration: 36000)\n","Processing Euler weight: 0037000.weight (Iteration: 37000)\n","Processing Euler weight: 0038000.weight (Iteration: 38000)\n","Processing Euler weight: 0039000.weight (Iteration: 39000)\n","Processing Euler weight: 0040000.weight (Iteration: 40000)\n","Processing Euler weight: 0041000.weight (Iteration: 41000)\n","Processing Euler weight: 0042000.weight (Iteration: 42000)\n","Processing Euler weight: 0043000.weight (Iteration: 43000)\n","Processing Euler weight: 0044000.weight (Iteration: 44000)\n","Processing Euler weight: 0045000.weight (Iteration: 45000)\n","Processing Euler weight: 0046000.weight (Iteration: 46000)\n","Processing Euler weight: 0047000.weight (Iteration: 47000)\n","Processing Euler weight: 0048000.weight (Iteration: 48000)\n","Processing Euler weight: 0049000.weight (Iteration: 49000)\n","Processing Euler weight: 0050000.weight (Iteration: 50000)\n","Processing Euler weight: 0051000.weight (Iteration: 51000)\n","Euler loss plot saved to /content/project645/results_plot_loss/euler_model_loss.png\n","\n","--- Euler loss plotting script finished. --- \n","Plot should be saved in: /content/project645/results_plot_loss\n"]}]}]}